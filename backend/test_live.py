#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒØ´Ù Ù…Ø¹ ÙƒØ§Ù…ÙŠØ±Ø§ MacBook Ø£Ùˆ ÙÙŠØ¯ÙŠÙˆ
========================================
"""

import cv2
import time
import sys
import os

# Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_webcam():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ ÙƒØ§Ù…ÙŠØ±Ø§ MacBook"""
    print("ğŸ¥ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒØ´Ù Ù…Ø¹ ÙƒØ§Ù…ÙŠØ±Ø§ MacBook")
    print("=" * 50)
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    try:
        from ultralytics import YOLO
        model = YOLO('./models/best.pt')
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
        print(f"   Ø§Ù„ÙØ¦Ø§Øª: {model.names}")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
        return
    
    # ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
    print("\nğŸ“¹ Ø¬Ø§Ø±ÙŠ ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§...")
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒØ§Ù…ÙŠØ±Ø§")
        print("   Ø¬Ø±Ø¨ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ù…Ù† Terminal Ù…Ø¨Ø§Ø´Ø±Ø©")
        return
    
    # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    
    print("âœ… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø¬Ø§Ù‡Ø²Ø©!")
    print("\nâŒ¨ï¸ Ø§Ù„Ø£ÙˆØ§Ù…Ø±:")
    print("   - Ø§Ø¶ØºØ· 'q' Ù„Ù„Ø®Ø±ÙˆØ¬")
    print("   - Ø§Ø¶ØºØ· 's' Ù„Ø­ÙØ¸ Ù„Ù‚Ø·Ø©")
    print("   - Ø§Ø¶ØºØ· 'd' Ù„ØªÙØ¹ÙŠÙ„/Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ÙƒØ´Ù")
    print()
    
    detection_enabled = True
    frame_count = 0
    detection_count = 0
    fps_start = time.time()
    fps = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø·Ø§Ø±")
            break
        
        frame_count += 1
        
        # Ø­Ø³Ø§Ø¨ FPS
        if frame_count % 30 == 0:
            fps = 30 / (time.time() - fps_start)
            fps_start = time.time()
        
        # Ø§Ù„ÙƒØ´Ù
        if detection_enabled and frame_count % 3 == 0:  # ÙƒÙ„ 3 Ø¥Ø·Ø§Ø±Ø§Øª
            results = model(frame, conf=0.5, device='mps', verbose=False)
            
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        # Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        conf = float(box.conf[0])
                        cls = int(box.cls[0])
                        name = model.names[cls]
                        
                        # Ø§Ù„Ù„ÙˆÙ† Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
                        color = (0, 0, 255) if 'hand' in name.lower() else (0, 128, 255)
                        
                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
                        
                        # Ø§Ù„Ù†Øµ
                        label = f"{name}: {conf:.0%}"
                        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
                        cv2.rectangle(frame, (x1, y1-h-10), (x1+w+10, y1), color, -1)
                        cv2.putText(frame, label, (x1+5, y1-5), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)
                        
                        detection_count += 1
                        print(f"ğŸš¨ ÙƒØ´Ù: {name} - Ø§Ù„Ø«Ù‚Ø©: {conf:.0%}")
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
        info = f"FPS: {fps:.1f} | Detections: {detection_count} | Detection: {'ON' if detection_enabled else 'OFF'}"
        cv2.putText(frame, info, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        # Ø¹Ø±Ø¶
        cv2.imshow('Nazra Detection Test - Press Q to quit', frame)
        
        # Ø§Ù„Ø£ÙˆØ§Ù…Ø±
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            break
        elif key == ord('s'):
            filename = f"snapshot_{int(time.time())}.jpg"
            cv2.imwrite(filename, frame)
            print(f"ğŸ“¸ ØªÙ… Ø­ÙØ¸: {filename}")
        elif key == ord('d'):
            detection_enabled = not detection_enabled
            print(f"ğŸ”„ Ø§Ù„ÙƒØ´Ù: {'Ù…ÙØ¹Ù‘Ù„' if detection_enabled else 'Ù…Ø¹Ø·Ù‘Ù„'}")
    
    cap.release()
    cv2.destroyAllWindows()
    print(f"\nğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
    print(f"   - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª: {frame_count}")
    print(f"   - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒØ´ÙˆÙØ§Øª: {detection_count}")


def test_video(video_path: str):
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ù…Ù„Ù ÙÙŠØ¯ÙŠÙˆ"""
    print(f"ğŸ¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒØ´Ù Ù…Ø¹ ÙÙŠØ¯ÙŠÙˆ: {video_path}")
    print("=" * 50)
    
    if not os.path.exists(video_path):
        print(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {video_path}")
        return
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    try:
        from ultralytics import YOLO
        model = YOLO('./models/best.pt')
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")
        return
    
    # ÙØªØ­ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("âŒ ÙØ´Ù„ ÙØªØ­ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")
        return
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    print(f"ğŸ“¹ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {total_frames} Ø¥Ø·Ø§Ø± @ {fps} FPS")
    
    frame_count = 0
    detection_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        
        # Ø§Ù„ÙƒØ´Ù ÙƒÙ„ 5 Ø¥Ø·Ø§Ø±Ø§Øª
        if frame_count % 5 == 0:
            results = model(frame, conf=0.5, device='mps', verbose=False)
            
            for result in results:
                if result.boxes is not None:
                    for box in result.boxes:
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        conf = float(box.conf[0])
                        cls = int(box.cls[0])
                        name = model.names[cls]
                        
                        color = (0, 0, 255) if 'hand' in name.lower() else (0, 128, 255)
                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
                        
                        label = f"{name}: {conf:.0%}"
                        cv2.putText(frame, label, (x1, y1-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
                        
                        detection_count += 1
                        print(f"ğŸš¨ Frame {frame_count}: {name} ({conf:.0%})")
        
        # Progress
        progress = f"Frame: {frame_count}/{total_frames}"
        cv2.putText(frame, progress, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)
        
        cv2.imshow('Video Detection - Press Q to quit', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    print(f"\nğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {detection_count} ÙƒØ´Ù ÙÙŠ {frame_count} Ø¥Ø·Ø§Ø±")


def test_image(image_path: str):
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ ØµÙˆØ±Ø©"""
    print(f"ğŸ–¼ï¸ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒØ´Ù Ù…Ø¹ ØµÙˆØ±Ø©: {image_path}")
    
    if not os.path.exists(image_path):
        print(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return
    
    from ultralytics import YOLO
    model = YOLO('./models/best.pt')
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ØµÙˆØ±Ø©
    img = cv2.imread(image_path)
    
    # Ø§Ù„ÙƒØ´Ù
    results = model(img, conf=0.5, device='mps')
    
    # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    annotated = results[0].plot()
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
    output_path = image_path.replace('.', '_detected.')
    cv2.imwrite(output_path, annotated)
    print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©: {output_path}")
    
    # Ø¹Ø±Ø¶
    cv2.imshow('Detection Result - Press any key', annotated)
    cv2.waitKey(0)
    cv2.destroyAllWindows()


if __name__ == "__main__":
    print("\n" + "=" * 50)
    print("   ğŸ¯ Ù†Ø¸Ø§Ù… Ù†Ø¸Ø±Ø© - Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙƒØ´Ù")
    print("=" * 50 + "\n")
    
    print("Ø§Ø®ØªØ± ÙˆØ¶Ø¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
    print("1. ÙƒØ§Ù…ÙŠØ±Ø§ MacBook (Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©)")
    print("2. Ù…Ù„Ù ÙÙŠØ¯ÙŠÙˆ")
    print("3. ØµÙˆØ±Ø©")
    print()
    
    choice = input("Ø§Ø®ØªÙŠØ§Ø±Ùƒ (1/2/3): ").strip()
    
    if choice == "1":
        test_webcam()
    elif choice == "2":
        path = input("Ù…Ø³Ø§Ø± Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: ").strip()
        if path:
            test_video(path)
        else:
            print("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø³Ø§Ø±")
    elif choice == "3":
        path = input("Ù…Ø³Ø§Ø± Ø§Ù„ØµÙˆØ±Ø©: ").strip()
        if path:
            test_image(path)
        else:
            print("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø³Ø§Ø±")
    else:
        print("Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­")

"""
Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© - Multi-Camera Management System
================================================================

Best Practices:
1. Thread per camera for reading (I/O bound)
2. Shared GPU workers for detection (compute bound)
3. Priority queue for frame processing
4. Smart frame skipping per camera
5. Batch processing when possible
6. WebSocket broadcasting for alerts

Architecture:
- CameraReader: Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª (thread per camera)
- FrameQueue: Ø·Ø§Ø¨ÙˆØ± Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ù…Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
- DetectionPool: Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† workers Ù„Ù„ÙƒØ´Ù
- AlertBroadcaster: Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
"""

import asyncio
import time
import threading
import queue
from typing import Dict, Optional, List, Callable, Awaitable, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import cv2
import numpy as np
from datetime import datetime
import logging

logger = logging.getLogger("Ù†Ø¸Ø±Ø©.Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª")


class CameraStatus(Enum):
    """Ø­Ø§Ù„Ø§Øª Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"""
    DISCONNECTED = "disconnected"
    CONNECTING = "connecting"
    CONNECTED = "connected"
    STREAMING = "streaming"
    ERROR = "error"
    PAUSED = "paused"


class FramePriority(Enum):
    """Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª"""
    HIGH = 1      # ÙƒØ§Ù…ÙŠØ±Ø§ Ø°Ø§Øª Ø£Ù‡Ù…ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø£Ùˆ ØªÙ†Ø¨ÙŠÙ‡ Ø³Ø§Ø¨Ù‚
    NORMAL = 2    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø§Ø¯ÙŠØ©
    LOW = 3       # ÙƒØ§Ù…ÙŠØ±Ø§ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©


@dataclass
class CameraConfig:
    """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"""
    camera_id: str
    name: str
    rtsp_url: str
    
    # Processing settings
    target_fps: int = 30
    detection_fps: int = 6       # ÙƒØ´Ù 6 Ù…Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©
    skip_frames: int = 5         # ØªØ®Ø·ÙŠ 5 Ø¥Ø·Ø§Ø±Ø§Øª Ø¨ÙŠÙ† ÙƒÙ„ ÙƒØ´Ù
    
    # Quality
    detection_scale: float = 0.5  # ØªØµØºÙŠØ± Ù„Ù„ÙƒØ´Ù Ø§Ù„Ø£Ø³Ø±Ø¹
    
    # Priority
    priority: FramePriority = FramePriority.NORMAL
    
    # Zone of Interest (optional)
    roi: Optional[Tuple[int, int, int, int]] = None  # x1, y1, x2, y2
    
    # Alert settings
    alert_on_detection: bool = True
    min_alert_interval: float = 5.0  # Ø«ÙˆØ§Ù†ÙŠ Ø¨ÙŠÙ† Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª


@dataclass
class FramePacket:
    """Ø­Ø²Ù…Ø© Ø¥Ø·Ø§Ø± Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    camera_id: str
    frame: np.ndarray
    timestamp: float
    frame_number: int
    priority: FramePriority
    config: CameraConfig
    
    def __lt__(self, other):
        """Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© ÙÙŠ priority queue"""
        if self.priority.value != other.priority.value:
            return self.priority.value < other.priority.value
        return self.timestamp < other.timestamp


@dataclass
class DetectionResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙƒØ´Ù"""
    camera_id: str
    frame_number: int
    timestamp: float
    detections: List[Dict]
    processing_time: float
    annotated_frame: Optional[np.ndarray] = None


@dataclass
class CameraState:
    """Ø­Ø§Ù„Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§"""
    config: CameraConfig
    status: CameraStatus = CameraStatus.DISCONNECTED
    
    # Statistics
    frames_read: int = 0
    frames_processed: int = 0
    detections_count: int = 0
    
    # Timing
    last_frame_time: float = 0.0
    last_detection_time: float = 0.0
    last_alert_time: float = 0.0
    
    # FPS tracking
    fps_read: float = 0.0
    fps_processed: float = 0.0
    
    # Current detections
    current_detections: List[Dict] = field(default_factory=list)
    
    # Last frame for display
    last_frame: Optional[np.ndarray] = None


class CameraReader(threading.Thread):
    """
    Ù‚Ø§Ø±Ø¦ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ - Thread Ù…Ø³ØªÙ‚Ù„ Ù„ÙƒÙ„ ÙƒØ§Ù…ÙŠØ±Ø§
    
    ÙŠÙ‚Ø±Ø£ Ø§Ù„Ø¥Ø·Ø§Ø±Ø§Øª Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± ÙˆÙŠØ¶ÙŠÙÙ‡Ø§ Ù„Ù„Ø·Ø§Ø¨ÙˆØ±
    """
    
    def __init__(
        self,
        config: CameraConfig,
        frame_queue: queue.PriorityQueue,
        state: CameraState,
        stop_event: threading.Event
    ):
        super().__init__(daemon=True, name=f"CameraReader-{config.camera_id}")
        self.config = config
        self.frame_queue = frame_queue
        self.state = state
        self.stop_event = stop_event
        self.cap: Optional[cv2.VideoCapture] = None
        self.frame_count = 0
        
    def run(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        logger.info(f"ğŸ“¹ Ø¨Ø¯Ø¡ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§: {self.config.name}")
        
        self.state.status = CameraStatus.CONNECTING
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
        self.cap = cv2.VideoCapture(self.config.rtsp_url)
        
        if not self.cap.isOpened():
            logger.error(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§: {self.config.name}")
            self.state.status = CameraStatus.ERROR
            return
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙÙŠØ¯ÙŠÙˆ
        actual_fps = self.cap.get(cv2.CAP_PROP_FPS) or 30
        width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        logger.info(f"âœ… Ù…ØªØµÙ„: {self.config.name} ({width}x{height} @ {actual_fps} FPS)")
        self.state.status = CameraStatus.STREAMING
        
        frame_interval = 1.0 / self.config.target_fps
        fps_counter = deque(maxlen=30)
        
        while not self.stop_event.is_set():
            start_time = time.time()
            
            try:
                ret, frame = self.cap.read()
                
                if not ret or frame is None:
                    logger.warning(f"âš ï¸ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Ø¥Ø·Ø§Ø± Ù…Ù†: {self.config.name}")
                    time.sleep(0.1)
                    continue
                
                self.frame_count += 1
                self.state.frames_read += 1
                self.state.last_frame = frame
                self.state.last_frame_time = time.time()
                
                # Ø­Ø³Ø§Ø¨ FPS
                fps_counter.append(time.time())
                if len(fps_counter) >= 2:
                    self.state.fps_read = len(fps_counter) / (fps_counter[-1] - fps_counter[0])
                
                # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø·Ø§Ø¨ÙˆØ± ÙÙ‚Ø· ÙƒÙ„ N Ø¥Ø·Ø§Ø±Ø§Øª (Ù„Ù„ÙƒØ´Ù)
                if self.frame_count % self.config.skip_frames == 0:
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù…ØªÙ„Ø§Ø¡ Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
                    if self.frame_queue.qsize() < 100:  # Ø­Ø¯ Ø£Ù‚ØµÙ‰
                        packet = FramePacket(
                            camera_id=self.config.camera_id,
                            frame=frame.copy(),
                            timestamp=time.time(),
                            frame_number=self.frame_count,
                            priority=self.config.priority,
                            config=self.config
                        )
                        self.frame_queue.put((packet.priority.value, packet))
                
                # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
                elapsed = time.time() - start_time
                sleep_time = frame_interval - elapsed
                if sleep_time > 0:
                    time.sleep(sleep_time)
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ {self.config.name}: {e}")
                time.sleep(0.5)
        
        # Ø¥ØºÙ„Ø§Ù‚
        if self.cap:
            self.cap.release()
        self.state.status = CameraStatus.DISCONNECTED
        logger.info(f"â¹ï¸ ØªÙˆÙ‚Ù Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§: {self.config.name}")


class MultiCameraProcessor:
    """
    Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
    
    Features:
    - Ø¥Ø¯Ø§Ø±Ø© ÙƒØ§Ù…ÙŠØ±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
    - Ø·Ø§Ø¨ÙˆØ± Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø¥Ø·Ø§Ø±Ø§Øª
    - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ø¹Ù„Ù‰ GPU
    - ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø­Ù…Ù„ Ø§Ù„Ø°ÙƒÙŠ
    """
    
    def __init__(
        self,
        detector,  # WeaponDetector instance
        max_cameras: int = 8,
        detection_workers: int = 2,
        max_queue_size: int = 100
    ):
        self.detector = detector
        self.max_cameras = max_cameras
        self.detection_workers = detection_workers
        
        # State
        self.cameras: Dict[str, CameraState] = {}
        self.readers: Dict[str, CameraReader] = {}
        self.stop_events: Dict[str, threading.Event] = {}
        
        # Shared frame queue with priority
        self.frame_queue: queue.PriorityQueue = queue.PriorityQueue(maxsize=max_queue_size)
        
        # Detection results queue
        self.results_queue: asyncio.Queue = asyncio.Queue()
        
        # Worker pool for detection
        self.detection_pool = ThreadPoolExecutor(
            max_workers=detection_workers,
            thread_name_prefix="detector"
        )
        
        # Control
        self.is_running = False
        self._processing_task: Optional[asyncio.Task] = None
        
        # Callbacks
        self.on_detection: Optional[Callable[[str, DetectionResult], Awaitable[None]]] = None
        self.on_alert: Optional[Callable[[str, Dict], Awaitable[None]]] = None
        self.on_frame: Optional[Callable[[str, np.ndarray, List[Dict]], Awaitable[None]]] = None
        
        # Statistics
        self.total_frames_processed = 0
        self.total_detections = 0
        self.start_time = time.time()
        
        logger.info(f"ğŸ¬ ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©")
        logger.info(f"   - Ø£Ù‚ØµÙ‰ ÙƒØ§Ù…ÙŠØ±Ø§Øª: {max_cameras}")
        logger.info(f"   - workers Ù„Ù„ÙƒØ´Ù: {detection_workers}")

    async def add_camera(self, config: CameraConfig) -> bool:
        """Ø¥Ø¶Ø§ÙØ© ÙƒØ§Ù…ÙŠØ±Ø§ Ø¬Ø¯ÙŠØ¯Ø©"""
        if config.camera_id in self.cameras:
            logger.warning(f"âš ï¸ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ù…ÙˆØ¬ÙˆØ¯Ø©: {config.camera_id}")
            return False
        
        if len(self.cameras) >= self.max_cameras:
            logger.error(f"âŒ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù…Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª: {self.max_cameras}")
            return False
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø­Ø§Ù„Ø©
        state = CameraState(config=config)
        self.cameras[config.camera_id] = state
        
        # Ø¥Ù†Ø´Ø§Ø¡ stop event
        stop_event = threading.Event()
        self.stop_events[config.camera_id] = stop_event
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù‚Ø§Ø±Ø¦
        reader = CameraReader(config, self.frame_queue, state, stop_event)
        self.readers[config.camera_id] = reader
        reader.start()
        
        logger.info(f"âœ… ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§: {config.name}")
        return True

    async def remove_camera(self, camera_id: str) -> bool:
        """Ø¥Ø²Ø§Ù„Ø© ÙƒØ§Ù…ÙŠØ±Ø§"""
        if camera_id not in self.cameras:
            return False
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù‚Ø§Ø±Ø¦
        if camera_id in self.stop_events:
            self.stop_events[camera_id].set()
        
        if camera_id in self.readers:
            self.readers[camera_id].join(timeout=2.0)
            del self.readers[camera_id]
        
        # ØªÙ†Ø¸ÙŠÙ
        if camera_id in self.stop_events:
            del self.stop_events[camera_id]
        if camera_id in self.cameras:
            del self.cameras[camera_id]
        
        logger.info(f"ğŸ—‘ï¸ ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§: {camera_id}")
        return True

    async def start(self):
        """Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        if self.is_running:
            return
        
        self.is_running = True
        self.start_time = time.time()
        
        # Ø¨Ø¯Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        self._processing_task = asyncio.create_task(self._processing_loop())
        
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª")

    async def stop(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        self.is_running = False
        
        # Ø¥ÙŠÙ‚Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚Ø±Ø§Ø¡
        for event in self.stop_events.values():
            event.set()
        
        for reader in self.readers.values():
            reader.join(timeout=2.0)
        
        # Ø¥ÙŠÙ‚Ø§Ù Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
        if self._processing_task:
            self._processing_task.cancel()
            try:
                await self._processing_task
            except asyncio.CancelledError:
                pass
        
        # Ø¥ÙŠÙ‚Ø§Ù pool
        self.detection_pool.shutdown(wait=False)
        
        logger.info("â¹ï¸ ØªÙˆÙ‚Ù Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª")

    async def _processing_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        logger.info("ğŸ”„ Ø¨Ø¯Ø¡ Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        
        while self.is_running:
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ù…Ù† Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
                try:
                    priority, packet = self.frame_queue.get(timeout=0.1)
                except queue.Empty:
                    await asyncio.sleep(0.01)
                    continue
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø·Ø§Ø±
                result = await self._process_frame(packet)
                
                if result:
                    self.total_frames_processed += 1
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©
                    camera_state = self.cameras.get(packet.camera_id)
                    if camera_state:
                        camera_state.frames_processed += 1
                        camera_state.last_detection_time = time.time()
                        camera_state.current_detections = result.detections
                        
                        if result.detections:
                            camera_state.detections_count += len(result.detections)
                            self.total_detections += len(result.detections)
                    
                    # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ callbacks
                    if result.detections:
                        if self.on_detection:
                            await self.on_detection(packet.camera_id, result)
                        
                        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡
                        await self._check_alert(packet.camera_id, result)
                    
                    if self.on_frame and result.annotated_frame is not None:
                        await self.on_frame(
                            packet.camera_id,
                            result.annotated_frame,
                            result.detections
                        )
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {e}")
                import traceback
                traceback.print_exc()
                await asyncio.sleep(0.1)

    async def _process_frame(self, packet: FramePacket) -> Optional[DetectionResult]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ø·Ø§Ø± ÙˆØ§Ø­Ø¯"""
        start_time = time.time()
        
        frame = packet.frame
        config = packet.config
        
        # ØªØµØºÙŠØ± Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„ÙƒØ´Ù
        if config.detection_scale != 1.0:
            detect_frame = cv2.resize(
                frame,
                None,
                fx=config.detection_scale,
                fy=config.detection_scale,
                interpolation=cv2.INTER_LINEAR
            )
        else:
            detect_frame = frame
        
        # ØªØ·Ø¨ÙŠÙ‚ ROI Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯
        if config.roi:
            x1, y1, x2, y2 = config.roi
            detect_frame = detect_frame[y1:y2, x1:x2]
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ´Ù
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                self.detection_pool,
                lambda: self.detector.detect_sync(
                    detect_frame,
                    f"{packet.camera_id}_{packet.frame_number}"
                )
            )
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙƒØ´Ù: {e}")
            return None
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¥Ø°Ø§ ØªÙ… Ø§Ù„ØªØµØºÙŠØ± Ø£Ùˆ ROI
        detections = []
        for det in result.detections:
            det_dict = {
                'id': det.id,
                'class_name': det.class_name,
                'class_name_ar': det.class_name_ar,
                'confidence': det.confidence,
                'severity': det.severity,
                'detection_type': det.detection_type,
            }
            
            # ØªØ­ÙˆÙŠÙ„ bbox
            bbox = det.bbox
            if isinstance(bbox, tuple):
                x1, y1, x2, y2 = bbox
            else:
                x1, y1 = bbox['x1'], bbox['y1']
                x2, y2 = bbox['x2'], bbox['y2']
            
            # ØªØ¹Ø¯ÙŠÙ„ Ù„Ù„Ù€ scale
            if config.detection_scale != 1.0:
                scale = 1.0 / config.detection_scale
                x1, y1, x2, y2 = int(x1*scale), int(y1*scale), int(x2*scale), int(y2*scale)
            
            # ØªØ¹Ø¯ÙŠÙ„ Ù„Ù„Ù€ ROI
            if config.roi:
                roi_x1, roi_y1, _, _ = config.roi
                x1 += roi_x1
                y1 += roi_y1
                x2 += roi_x1
                y2 += roi_y1
            
            det_dict['bbox'] = {'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2}
            detections.append(det_dict)
        
        # Ø±Ø³Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø£ØµÙ„ÙŠ
        annotated_frame = self._draw_detections(frame.copy(), detections)
        
        processing_time = time.time() - start_time
        
        return DetectionResult(
            camera_id=packet.camera_id,
            frame_number=packet.frame_number,
            timestamp=packet.timestamp,
            detections=detections,
            processing_time=processing_time,
            annotated_frame=annotated_frame
        )

    async def _check_alert(self, camera_id: str, result: DetectionResult):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡"""
        camera_state = self.cameras.get(camera_id)
        if not camera_state:
            return
        
        config = camera_state.config
        if not config.alert_on_detection:
            return
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙØ§ØµÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ
        current_time = time.time()
        if current_time - camera_state.last_alert_time < config.min_alert_interval:
            return
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡
        for det in result.detections:
            if det.get('severity') == 'critical':
                camera_state.last_alert_time = current_time
                
                if self.on_alert:
                    alert = {
                        'camera_id': camera_id,
                        'camera_name': config.name,
                        'detection': det,
                        'timestamp': datetime.utcnow().isoformat(),
                        'frame_number': result.frame_number
                    }
                    await self.on_alert(camera_id, alert)
                    logger.warning(f"ğŸš¨ ØªÙ†Ø¨ÙŠÙ‡ Ù…Ù† {config.name}: {det['class_name_ar']}")

    def _draw_detections(self, frame: np.ndarray, detections: List[Dict]) -> np.ndarray:
        """Ø±Ø³Ù… Ø§Ù„ÙƒØ´ÙˆÙØ§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ø§Ø±"""
        for det in detections:
            bbox = det['bbox']
            x1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']
            
            # Ø§Ù„Ù„ÙˆÙ† Ø­Ø³Ø¨ Ø§Ù„Ø®Ø·ÙˆØ±Ø©
            severity = det.get('severity', 'low')
            colors = {
                'critical': (0, 0, 255),   # Ø£Ø­Ù…Ø±
                'high': (0, 128, 255),     # Ø¨Ø±ØªÙ‚Ø§Ù„ÙŠ
                'medium': (0, 255, 255),   # Ø£ØµÙØ±
                'low': (0, 255, 0)         # Ø£Ø®Ø¶Ø±
            }
            color = colors.get(severity, (255, 255, 255))
            
            # Ø±Ø³Ù… Ø§Ù„Ù…Ø±Ø¨Ø¹
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            
            # Ø§Ù„Ù†Øµ
            confidence = det.get('confidence', 0) * 100
            label = f"{det.get('class_name_ar', 'unknown')}: {confidence:.0f}%"
            
            # Ø®Ù„ÙÙŠØ© Ù„Ù„Ù†Øµ
            (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            cv2.rectangle(frame, (x1, y1-text_h-10), (x1+text_w+10, y1), color, -1)
            cv2.putText(frame, label, (x1+5, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)
        
        return frame

    def get_stats(self) -> Dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø©"""
        uptime = time.time() - self.start_time
        
        cameras_stats = {}
        for cam_id, state in self.cameras.items():
            cameras_stats[cam_id] = {
                'name': state.config.name,
                'status': state.status.value,
                'fps_read': round(state.fps_read, 1),
                'frames_read': state.frames_read,
                'frames_processed': state.frames_processed,
                'detections_count': state.detections_count,
                'current_detections': len(state.current_detections),
            }
        
        return {
            'uptime_seconds': round(uptime, 1),
            'total_cameras': len(self.cameras),
            'active_cameras': sum(1 for s in self.cameras.values() if s.status == CameraStatus.STREAMING),
            'total_frames_processed': self.total_frames_processed,
            'total_detections': self.total_detections,
            'queue_size': self.frame_queue.qsize(),
            'avg_fps': round(self.total_frames_processed / uptime, 1) if uptime > 0 else 0,
            'cameras': cameras_stats
        }

    def get_camera_frame(self, camera_id: str) -> Optional[Tuple[np.ndarray, List[Dict]]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ø¥Ø·Ø§Ø± Ù…Ø¹ Ø§Ù„ÙƒØ´ÙˆÙØ§Øª"""
        state = self.cameras.get(camera_id)
        if not state or state.last_frame is None:
            return None
        
        return state.last_frame, state.current_detections


# ===========================================
# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
# ===========================================
"""
from app.services.detector import WeaponDetector

async def main():
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ø´Ù
    detector = WeaponDetector()
    await detector.load_model()
    
    # ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§Øª
    processor = MultiCameraProcessor(
        detector=detector,
        max_cameras=4,
        detection_workers=2
    )
    
    # Callbacks
    async def on_alert(camera_id, alert):
        print(f"ğŸš¨ ØªÙ†Ø¨ÙŠÙ‡: {alert}")
        # Ø¥Ø±Ø³Ø§Ù„ Ø¹Ø¨Ø± WebSocket
        # await websocket_manager.broadcast(alert)
    
    processor.on_alert = on_alert
    
    # Ø¥Ø¶Ø§ÙØ© ÙƒØ§Ù…ÙŠØ±Ø§Øª
    await processor.add_camera(CameraConfig(
        camera_id="cam_1",
        name="Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ",
        rtsp_url="rtsp://192.168.1.100:554/stream",
        priority=FramePriority.HIGH,
        detection_fps=10  # ÙƒØ´Ù Ø£Ø¹Ù„Ù‰ Ù„Ù„Ù…Ø¯Ø®Ù„
    ))
    
    await processor.add_camera(CameraConfig(
        camera_id="cam_2",
        name="Ù…ÙˆÙ‚Ù Ø§Ù„Ø³ÙŠØ§Ø±Ø§Øª",
        rtsp_url="rtsp://192.168.1.101:554/stream",
        priority=FramePriority.NORMAL,
        detection_fps=6
    ))
    
    # Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    await processor.start()
    
    # ØªØ´ØºÙŠÙ„ Ù„Ù…Ø¯Ø© Ù…Ø¹ÙŠÙ†Ø©
    await asyncio.sleep(60)
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    print(processor.get_stats())
    
    # Ø¥ÙŠÙ‚Ø§Ù
    await processor.stop()

if __name__ == "__main__":
    asyncio.run(main())
"""
